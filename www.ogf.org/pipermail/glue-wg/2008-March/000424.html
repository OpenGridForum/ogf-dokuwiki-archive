<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 
<!-- Mirrored from www.ogf.org/pipermail/glue-wg/2008-March/000424.html by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 21 Oct 2022 15:01:33 GMT -->
<HEAD>
   <TITLE> [glue-wg] Some thoughts on storage objects
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:glue-wg%40ogf.org?Subject=%5Bglue-wg%5D%20Some%20thoughts%20on%20storage%20objects&In-Reply-To=200803272206.44747.paul.millar%40desy.de">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="https://www.ogf.org/pipermail/glue-wg/2008-March/000415.html">
   <LINK REL="Next"  HREF="https://www.ogf.org/pipermail/glue-wg/2008-March/000431.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[glue-wg] Some thoughts on storage objects</H1>
    <B>Maarten.Litmaath at cern.ch</B> 
    <A HREF="mailto:glue-wg%40ogf.org?Subject=%5Bglue-wg%5D%20Some%20thoughts%20on%20storage%20objects&In-Reply-To=200803272206.44747.paul.millar%40desy.de"
       TITLE="[glue-wg] Some thoughts on storage objects">Maarten.Litmaath at cern.ch
       </A><BR>
    <I>Sun Mar 30 18:48:16 CDT 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="https://www.ogf.org/pipermail/glue-wg/2008-March/000415.html">[glue-wg] Some thoughts on storage objects
</A></li>
        <LI>Next message: <A HREF="https://www.ogf.org/pipermail/glue-wg/2008-March/000431.html">[glue-wg] Some thoughts on storage objects
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="https://www.ogf.org/pipermail/glue-wg/2008-March/date.html#424">[ date ]</a>
              <a href="https://www.ogf.org/pipermail/glue-wg/2008-March/thread.html#424">[ thread ]</a>
              <a href="https://www.ogf.org/pipermail/glue-wg/2008-March/subject.html#424">[ subject ]</a>
              <a href="https://www.ogf.org/pipermail/glue-wg/2008-March/author.html#424">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Hi Paul,
various comments inline.

&gt;<i> [...]
</I>&gt;<i> 
</I>&gt;<i> BTW, I'm implicitly assuming that StorageEnvironment.RetentionPolicy can be 
</I>&gt;<i> multivalued.  If this isn't true and we have the use-case of the same 
</I>&gt;<i> physical disks being part of, for example, both Custodial and Output storage, 
</I>&gt;<i> then it starts to get complicated.
</I>
I think it is OK if the RetentionPolicy _can_ be multivalued, but in WLCG
it would be published single-valued, viz. along with an AccessLatency to
describe the Storage Class that is implemented by the Environment.

&gt;<i> [...]
</I>&gt;<i> 
</I>&gt;<i> StorageEnvironment:
</I>&gt;<i> 
</I>&gt;<i>  A StorageEnvironment is a collection of one or more StorageCapacities
</I>&gt;<i>  with a set of associated (enforced) storage management policies.
</I>&gt;<i>  Examples of these policies are Type (Volatile, Durable, Permanent)
</I>&gt;<i>  and RetentionPolicy (Custodial, Output, Replica).
</I>
Note that we should get rid of the obsolete, confusing Type and Lifetime
attributes in favor of the ExpirationMode copied from SRM v3.

&gt;<i> [...]
</I>&gt;<i> 
</I>&gt;<i> StorageResource:
</I>&gt;<i> 
</I>&gt;<i>  A StorageResource is an aggregation of one or more
</I>&gt;<i>  StorageEnvironments and describes the hardware that a particular
</I>&gt;<i>  software instance has under its control.
</I>
See my reply to Sergio: we may rather want to allow an Environment
to be linked to multiple Resources, e.g. a disk and a tape Resource,
such that we can publish the back-end implementation name and version
for each of them.

&gt;<i>  A StorageResource must have at least one StorageEnvironment,
</I>&gt;<i>  otherwise there wouldn't be much point publishing information
</I>&gt;<i>  about it. [This isn't a strict requirement, but I think it makes sense
</I>&gt;<i>  to include it.]
</I>
OK.

&gt;<i>  All StorageEnvironments must be part of precisely one
</I>&gt;<i>  StorageResource.  SoftwareEnvironments may not be shared between
</I>&gt;<i>  StorageResources.  This means that all physics hardware must
</I>&gt;<i>  be published under precisely one StorageResource.
</I>
See my comment above.

&gt;<i> StorageShare:
</I>&gt;<i> 
</I>&gt;<i>  A StorageShare is a logical partitioning of one or more
</I>&gt;<i>  StorageEnvironments.
</I>&gt;<i> 
</I>&gt;<i>  Perhaps the simplest example of a StorageShare is one
</I>&gt;<i>  associated with a single StorageEnvironment with a single
</I>&gt;<i>  associated StorageCapacity, and that represents all
</I>&gt;<i>  the available storage of that StorageCapacity.  An example of
</I>&gt;<i>  a storage that could be represented by this trivial
</I>&gt;<i>  StorageShare is the classic-SE.
</I>&gt;<i> 
</I>&gt;<i>  StorageSpaces must have one or more associated StorageCapacities.
</I>|<i>         ^^^^^^
</I>|<i>         Shares
</I>
&gt;<i>  These StorageCapacities provide a complete description of the different
</I>&gt;<i>  homogeneous underlying technologies that are available under the space.
</I>&gt;<i> 
</I>&gt;<i>  In general, the number of StorageCapacities associated with a
</I>&gt;<i>  StorageShare is the sum of the number of StorageCapacities associated
</I>&gt;<i>  with each of the StorageShare's associated StorageEnvironments.
</I>&gt;<i> 
</I>&gt;<i>  Following from this, there is an implicit association between the
</I>&gt;<i>  StorageCapacity associated with a StorageShare and the corresponding
</I>&gt;<i>  StorageCapacity associated with a StorageEnvironment.  Intuitively, this
</I>&gt;<i>  association is from the fact that the two StorageCapacities share the
</I>&gt;<i>  same underlying physical storage.  This implicit association is not
</I>&gt;<i>  recorded in GLUE.
</I>&gt;<i> 
</I>&gt;<i>  StorageSpaces may overlap.  Specifically, given a StorageCapacity
</I>|<i>         ^^^^^^
</I>|<i>         Shares
</I>
&gt;<i>  (SC_E) that is associated with some StorageEnvironment and which has
</I>&gt;<i>  totalSize TS_E, let the sum of the totalSize attributes for all
</I>|<i>                  ^^^
</I>|<i>                  let TS_S be .....
</I>
&gt;<i>  StorageCapacities that are:
</I>&gt;<i> 	1. associated with a StorageSpace, and
</I>|<i>                                   ^^^^^
</I>|<i>                                   Share
</I>
&gt;<i> 	2. that are implicitly associated with SC_E
</I>&gt;<i>  be TS_S.  If the StorageSpaces are covering then TS_S = TS_E.  If
</I>|<i>  ^^^^^^^                 ^^^^^^
</I>|<i>  XXXXXXX                 Shares
</I>
&gt;<i>  the StorageSpaces overlap, then TS_S &gt; TS_E.
</I>|<i>             ^^^^^^
</I>|<i>             Shares
</I>
&gt;<i>   [sorry, I couldn't easily describe this with just words without it sounding
</I>&gt;<i>   awful!]
</I>&gt;<i> 
</I>&gt;<i>  StorageSpaces may be incomplete.  Following the same definitions
</I>|<i>         ^^^^^^
</I>|<i>         Shares
</I>
&gt;<i>  as above, this is when TS_S &lt; TS_E.  Intuitively, this happens if
</I>&gt;<i>  the site-admin has not yet assigned all available storage.
</I>&gt;<i> 
</I>&gt;<i>  End-users within a UserDomain may wish to store or retrieve files.  The
</I>&gt;<i>  StorageShares provides a complete, abstract description of the
</I>&gt;<i>  underlying storage at their disposal.  No member of a UserDomain may
</I>&gt;<i>  interact with the physical hardware except through a StorageShare.
</I>&gt;<i> 
</I>&gt;<i>  The partitioning is persistent through file creation and deletion.  The
</I>&gt;<i>  totalSize attributes (of a StorageSpace's associated StorageCapacties)
</I>|<i>                                    ^^^^^
</I>|<i>                                    Share
</I>
&gt;<i>  do not change as a result of file creation or deletion.  [Does GLUE need to
</I>&gt;<i>  stipulate this, or should we leave this vague?]
</I>
Why mention it at all?  You do not make statements about the behavior
of the other sizes, and I think there is no need to go there...

&gt;<i>  A single StorageShare may allow multiple UserDomains to access
</I>&gt;<i>  storage; if so, the StorageShare is &quot;shared&quot; between the different
</I>&gt;<i>  UserDomains.  Such a shared StorageShare is typical if a site
</I>&gt;<i>  provides storage described by the trivial StorageShare (one that
</I>&gt;<i>  covers a complete StorageEnvironment) whilst supporting multiple
</I>&gt;<i>  UserDomains.
</I>&gt;<i> 
</I>&gt;<i> [...]
</I>&gt;<i> 
</I>&gt;<i> StorageAccessProtocol:
</I>&gt;<i> 
</I>&gt;<i>  A StorageAccessProtocol describes how data may be sent or received.
</I>&gt;<i>  The presence of a StorageAccessProtocol indicates that data may be
</I>&gt;<i>  fetched or stored using this interface.
</I>&gt;<i> 
</I>&gt;<i>  Access to the interface may be localised; that is, only available
</I>&gt;<i>  from certain computers.  It may also be restricted to specified
</I>&gt;<i>  UserDomains.  However, neither policy restrictions are published in
</I>&gt;<i>  GLUE.  On observing a StorageAccessProcol, one may deduce only that
</I>&gt;<i>  it is valid for at least one user of one supported UserDomain.
</I>
..... from at least one computer.

Thanks,
	Maarten

</PRE>





<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="https://www.ogf.org/pipermail/glue-wg/2008-March/000415.html">[glue-wg] Some thoughts on storage objects
</A></li>
	<LI>Next message: <A HREF="https://www.ogf.org/pipermail/glue-wg/2008-March/000431.html">[glue-wg] Some thoughts on storage objects
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="https://www.ogf.org/pipermail/glue-wg/2008-March/date.html#424">[ date ]</a>
              <a href="https://www.ogf.org/pipermail/glue-wg/2008-March/thread.html#424">[ thread ]</a>
              <a href="https://www.ogf.org/pipermail/glue-wg/2008-March/subject.html#424">[ subject ]</a>
              <a href="https://www.ogf.org/pipermail/glue-wg/2008-March/author.html#424">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://www.ogf.org/mailman/listinfo/glue-wg">More information about the glue-wg
mailing list</a><br>
</body>
<!-- Mirrored from www.ogf.org/pipermail/glue-wg/2008-March/000424.html by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 21 Oct 2022 15:01:33 GMT -->
</html>
